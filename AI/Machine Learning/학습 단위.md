## 데이터 학습 단위

대규모 데이터셋을 효율적으로 학습하기 위해 데이터셋을 여러 단위로 나누어 사용한다.

1. Epoch

   > Epoch는 훈련 데이터셋에 포함된 모든 데이터셋이 한 번씩 신경망에 적용되어 모델을 통과한 횟수이다.

   이는 곧 모든 학습 데이터셋을 한 번 학습한 횟수를 뜻한다. epoch를 높일수록 다양한 무작위의 가중치로 학습을 하게 되므로, 적합한 파라미터를 찾을 확률이 높아진다. 하지만 이를 지나치게 높이게 된다면 그 데이터셋에 대해 overfitting(과적합)현상이 일어나서 다른 데이터에 대해 제대로 된 예측을 하지 못하게 될 수 있다.

2. Batch size

   Batch size는 전체 데이터셋, 즉 full batch를 여러 작은 그룹으로 분할했을 때, 하나의 그룹에 속하는 데이터의 수를 의미하며, 이 그룹을 batch 혹은 mini batch라고 한다. 즉 batch는 epoch를 구성하는 데이터셋의 부분집합인 것이다.

   이 batch size가 너무 큰 경우, 한 번에 처리해야 할 데이터의 양이 많아진다는 뜻이므로 비용이 커질 수 있다. 또한 너무 작은 경우, 적은 데이터로 가중치가 지나치게 자주 업데이트되어 훈련이 불안정해질 수 있다.

3. Iteration

   Iteration은 epoch 하나를 마치는데 필요한 mini batch의 수를 의미하며 이는 1 epoch를 마치는데 필요한 업데이트의 횟수라고도 할 수 있다. step이라고 부르기도 한다.

---
