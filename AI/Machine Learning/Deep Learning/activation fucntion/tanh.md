# 하이퍼볼릭탄젠트

1. 개념

   Tanh는 sigmoid의 한계를 보완하고 크기와 위치를 조정한 함수이다. 데이터의 -1과 1 사이의 비선형 형태 변환을 위해 사용한다. Tanh 함수는 입력값이 0일 때 0을 출력하고, 입력값이 양수일 때는 1에 가까운 값을, 음수일 때는 -1에 가까운 값을 출력한다. 이는 신경망의 출력값을 중심으로 대칭적인 분포를 가지게 하여 학습을 안정적으로 만든다.

   Tanh의 함수식은 다음과 같다.

   > $T(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$

   이 함수는 입력값에 대해 쌍곡선 탄젠트 값을 출력하며, 이는 -1과 1 사이의 값을 가진다.

2. 특징

   Tanh 함수는 Sigmoid 함수보다 출력값의 범위가 -1에서 1로 더 넓기 때문에 출력값의 진폭이 크다. 이로 인해 gradient vanishing(기울기 소실) 현상이 적게 한다는 장점이 있다. 또한, Tanh 함수는 입력값이 0에 가까울수록 기울기가 커지며, 입력값이 매우 크거나 작을 때 기울기가 0에 가까워지는 특성을 가진다. 따라서 hidden layer에서의 사용은 여전히 조심해야 한다.

3. 사용

   Tanh 함수는 Sigmoid 함수와 유사하게 사용되며, 주로 이진 분류 문제나 출력값이 -1과 1 사이의 값을 가지는 문제에서 사용된다. Tanh 함수는 신경망의 각 층에서 데이터의 분포를 중앙으로 모아주는 역할을 하여 학습을 더 빠르고 안정적으로 할 수 있게 도와준다. 그러나 gradient vanishing 문제를 완전히 해결하지는 못하므로, 깊은 신경망에서는 주의가 필요하다.
