## 리키 렐?루

1. 개념

   Leaky ReLU는 ReLU(Rectified Linear Unit)의 변형으로, 입력값이 음수일 때 작은 기울기를 가지는 활성화 함수이다. ReLU가 음수 입력에 대해 0을 출력하는 반면, Leaky ReLU는 작은 음수값을 출력한다. 일반적으로 이 기울기는 0.01로 설정되지만, 이는 하이퍼파라미터로 조정 가능하다.

   입력값이 양수이면 그대로 출력하고,

   > $f(x) = x$

   음수일 때는 기울기를 붙여 출력한다.

   > $f(x) = αx$

2. 특징

   - dying ReLU 문제 해결: ReLU의 주요 문제점인 뉴런이 '죽는' 현상을 완화한다. 음수 입력에 대해 작은 기울기를 허용함으로써 그래디언트가 흐를 수 있게 한다.
   - 정보 보존: 음수 입력에 대해서도 작은 값을 출력하므로, 네트워크의 깊이가 깊어져도 정보 손실을 줄일 수 있다.
   - 계산 효율성: ReLU와 마찬가지로 계산이 단순하여 연산 속도가 빠르다.
   - 비선형성: 함수의 비선형적 특성으로 인해 네트워크가 복잡한 패턴을 학습할 수 있게 한다.

3. 사용

   - 음수 입력이 중요한 정보를 담고 있을 가능성이 있는 경우에 사용한다.
   - ReLU로 학습이 잘 되지 않거나, 뉴런이 죽는 현상이 자주 발생하는 경우 대안으로 사용할 수 있다.
   - 특히 매우 깊은 신경망에서 그래디언트 소실 문제를 완화하기 위해 사용된다.

4. 주의사항

   - α 값의 선택이 중요하다. 너무 작으면 ReLU와 비슷해지고, 너무 크면 학습이 불안정해질 수 있다.
   - 여전히 음수 입력에 대한 출력이 작기 때문에, 완전히 음수 입력의 중요성을 살리지는 못할 수 있다.
