# 차원 축소와 시각화

> 차원 축소는 다차원으로 이루어진 데이터들의 차원을 축소하여 새로운 차원의 데이터를 생성하는 것이다.

1. 개념

   차원의 증가에 비례하여 데이터 간의 거리도 증가하기 때문에 고차원은 곧 학습의 신뢰도를 떨어뜨린다. 또한 일상적 경험의 3차원 물리적 공간 등 저차원적 환경에서 일어나지 않는 고차원적 공간에서 데이터를 분석하고 정리할 때 발생하는 다양한 현상인 curse of dimensionality(차원의 저주)등의 문제가 있다.

   차원이 증가하면 공간의 부피가 빠르게 증가한다. 따라서 사용 가능한 데이터가 희박해진다. 신뢰할 수 있는 결과를 얻기 위해 필요한 데이터의 양이 차원에 비례하여 기하급수적으로 증가하는 경향을 띈다. 따라서 고차원 데이터는 근본적으로 처리량 자체가 저차원에 비해 월등히 높다.

   따라서 차원 축소를 이용해 고차원인 원본 데이터의 의미 있는 특성을 이상적으로 원래의 차원에 가깝게 유지할 수 있도록 변환(the transformation of data from a high-dimensional space into a low-dimensional space)된 저차원 데이터(low-dimensional representation)를 학습함으로써 이러한 문제를 해결할 수 있다.

   하지만 차원 축소를 하게 된다면 필연적으로 원본 데이터로부터의 information loss가 발생하는데, 이러한 손실을 최소화하여 얼마나 잘 representation(재표현)할 수 있느냐가 차원 축소의 관건이다.

2. 종류

   1. Projection

      선형

      1. PCA

         PCA는 Principal Component Analysis의 약자로 주성분 분석이라는 뜻이다. 이름과 같이 데이터 분포의 주성분을 찾는 방법이다. 이때 주성분이란, 데이터의 분산이 가장 큰 방향벡터를 뜻한다. 쉽게 말해서 차원 축소를 통해 데이터를 가장 잘 설명할 수 있는 잠재적인 요소를 추출하는 것이다.

         데이터를 축에 사영시켰을 때, 분산이 크다는 것이 원래 데이터의 분포를 더 잘 설명할 수 있다는 것을 뜻한다. 이때 사영시키는 축이 이 알고리즘에서 찾으려고 하는 잠재적인 요소, 즉 주성분이다.

         이를 그림으로 표현하면 다음과 같다.

   2. Manifold Learning

      비선형
