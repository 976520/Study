# 경사하강법 탐구하기

> 경사하강법은 함수의 최솟값을 찾는 최적화 이론 기법이다.

~~인공신경망을 만만하게 본 학우들이 처음으로 갈려나가는 관문이다.~~

---

## 개념

1. 정의

   경사하강법이란 함수의 값이 낮아지는 방향으로 각 독립변수들의 값을 변형시키면서 함수가 최솟값을 갖도록 하는 독립변수의 값을 탐색하는 방법이다. 함수의 gradient(경사)를 구하고, 해당 gradient의 반대 방향으로 이동시킨다. 이 과정을 극값, 즉 최소 지점에 이를 때 까지 반복한다. 이를 반대로 최고 지점에 이를 때 까지 반복하면 경사상승법이라고 한다.

2. gradient

   여기서 gradient란 $R^n$ -> $R$ 형태의 실수 함수의 미분을 뜻한다.

   함수값이 실수인 $n$차원 다변수함수 $f(x)=f(x_1, x_2, ..., x_n)$가 있을 때, **$f(x)$를 각 변수에 대해 일차 편미분한 벡터**를 gradient라고 하며 매개변수가 $x$일 때 목적함수 $f$의 gradient 값을 $∇f(x)$와 같이 표현한다. 따라서 gradient는 $x$에 대해 $f(x)$가 가장 가파르게 증가하는 방향과 증가율을 나타낸다고 할 수 있다.

---

## 알고리즘

기본적인~~아주 이상적인 데이터의~~ 경사하강법의 알고리즘은 다음 순서를 거친다.

1. 시작점

   임의의 매개변수를 시작점으로 설정한다.

   이 시작점을 어디로 잡느냐에 따라 나중에 수렴 지점이 달라질 수 있다. $y$값이 가장 낮은 부분인 전역 최소점(global minima)을 목적으로 했지만, 미분계수가 가장 낮은 부분인 지역 최소점(local minima)으로 수렴할 가능성이 있다. 이러한 문제를 local minimum 문제라고 한다. 따라서 수렴 지점이 무조건 최소값이 아닐 수 있다. 이를 한짤요약하면 다음과 같다.

   ![Untitled](https://github.com/user-attachments/assets/bb8e8ebd-7dce-408b-8075-05cc73504846)

2. 업데이트

   최적화하고자 하는 함수 $f(x)$에 대해 다음과 같은 식을 이용해 업데이트를 한다. 이때 $x_t$는 $t$번째 반복의 매개변수 $x$값이다.

   > $\$x_{t+1}$ $=$ $x_t - η \frac{∂f}{∂x}(x_t)$

   이를 다변수함수에 대해 확장하면 다음과 같다.

   > $x_{t+1}$ $=$ $x_t - η∇f(x_t)$

   미분의 인수가 함수 $f$의 그것과 ~~편미분이니까 당연히~~같으므로, 함수 의존성을 명시적으로 표기하여 정의하자면 다음과 같다.

   > $∇f(x_1, x_2, ..., x_n) = (\frac{∂f}{∂x_1},\frac{∂f}{∂x_2},...,\frac{∂f}{∂x_n}) = \frac{∂f}{∂x}(x_1, x_2,...,x_n)$

   계수 $η$는 이동할 거리(step size)를 조정하는 하이퍼파라미터이며, 이를 학습률(learning rate)이라고 한다. 학습률이 너무 크면 수렴할 값을 놓치고 발산할 수 있고(overshooting), 학습률이 너무 작으면 그만큼 수렴이 늦어질 수 있다.

   다음은 각각 학습률이 너무 클때(overshooting)와 너무 낮을때(learn too slow)를 표현한 그림이다.

   ![Untitled2](https://github.com/user-attachments/assets/4d799835-d665-4575-af65-75ff74b592e9)

3. 반복

   목적함수가 특정 값으로 알맞게 수렴할 때 까지 이 과정을 반복한다. 인공신경망에서는 손실함수가 최소가 되는 지점을 찾기 위해 경사하강법을 사용한다.

---

## 종류

1. SGD (Stochastic Gradient Descent)

2. Momentum

   Momentum은 기존 경사하강법에 가속도항을 추가하여 local minimum 문제를 해결한 경사하강 방법론이며 이의 업데이트 식은 다음과 같다.

   > $v_{t+1}=v_t-η\frac{∂L}{∂W}$

   > $W_{t+1} = W_t + v_t$

   $v$는 일종의 가속도라고 생각하는 것이 이해가 편하다. $v$의 영향으로 인해 기존 가중치가 증가 혹은 감소하던 방향으로 더 크게 변화하게끔 하였다. 당연히 $v$는 처음에 0으로 초기화된다.

3. Adagrad

4. RMSProp

5. Adam
