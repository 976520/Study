# 경사하강법 탐구하기

> 경사하강법은 함수의 최솟값을 찾는 최적화 이론 기법이다.

~~인공신경망을 만만하게 본 학우들이 처음으로 갈려나가는 관문이다.~~

---

## 개념

1. 정의

   경사하강법이란 함수의 값이 낮아지는 방향으로 각 독립변수들의 값을 변형시키면서 함수가 최솟값을 갖도록 하는 독립변수의 값을 탐색하는 방법이다. 함수의 gradient(경사)를 구하고, 해당 gradient의 반대 방향으로 이동시킨다. 이 과정을 극값, 즉 최소 지점에 이를 때 까지 반복한다. 이를 반대로 최고 지점에 이를 때 까지 반복하면 경사상승법이라고 한다.

2. gradient

   여기서 gradient란 $R^n$ -> $R$ 형태의 실수 함수의 미분을 뜻한다.

   함수값이 실수인 $n$차원 다변수함수 $f(x)=f(x_1, x_2, ..., x_n)$가 있을 때, **$f(x)$를 각 변수에 대해 일차 편미분한 벡터**를 gradient라고 하며 매개변수가 $x$일 때 목적함수 $f$의 gradient 값을 $∇f(x)$와 같이 표현한다. 따라서 gradient는 $x$에 대해 $f(x)$가 가장 가파르게 증가하는 방향과 증가율을 나타낸다고 할 수 있다.

---

## 알고리즘

기본적인~~아주 이상적인 데이터의~~ 경사하강법의 알고리즘은 다음 순서를 거친다.

1. 시작점

   임의의 매개변수를 시작점으로 설정한다.

   이 시작점을 어디로 잡느냐에 따라 나중에 수렴 지점이 달라질 수 있다. $y$값이 가장 낮은 부분인 전역 최소점(global minima)을 목적으로 했지만, 미분계수가 가장 낮은 부분인 지역 최소점(local minima)으로 수렴할 가능성이 있다. 이러한 문제를 local minimum 문제라고 한다. 따라서 수렴 지점이 무조건 최소값이 아닐 수 있다.
   ![Untitled](https://github.com/user-attachments/assets/bb8e8ebd-7dce-408b-8075-05cc73504846)

2. 업데이트

   최적화하고자 하는 함수 $f(x)$에 대해 다음과 같은 식을 이용해 업데이트를 한다. 이때 $x_t$는 $t$번째 반복의 매개변수 $x$값이다.

   > $\$x_{t+1}$ $=$ $x_t - η \frac{∂f}{∂x}(x_t)$

   이를 다변수함수에 대해 확장하면 다음과 같다.

   > $x_{t+1}$ $=$ $x_t - η∇f(x_t)$

   미분의 인수가 함수 $f$의 그것과 ~~편미분이니까 당연히~~같으므로, 함수 의존성을 명시적으로 표기하여 정의하자면 다음과 같다.

   > $∇f(x_1, x_2, ..., x_n) = (\frac{∂f}{∂x_1},\frac{∂f}{∂x_2},...,\frac{∂f}{∂x_n}) = \frac{∂f}{∂x}(x_1, x_2,...,x_n)$

   이때 계수 $η$는 이동할 거리(step size)를 조정하는 하이퍼파라미터이며, 이를 학습률(learning rate)이라고 한다. 학습률이 너무 크면 수렴할 값을 놓치고 발산할 수 있고(overshooting), 학습률이 너무 작으면 그만큼 수렴이 늦어질 수 있다.

   다음은 각각 학습률이 너무 클때(overshooting)와 너무 낮을때(learn too slow)를 표현한 그림이다.

   ![Untitled2](https://github.com/user-attachments/assets/4d799835-d665-4575-af65-75ff74b592e9)

3. 반복

   목적함수가 특정 값으로 알맞게 수렴할 때 까지 이 과정을 반복한다. 인공신경망에서는 손실함수가 최소가 되는 지점을 찾기 위해 경사하강법을 사용한다.

---

## 종류

1. SGD (Stochastic Gradient Descent)

   SGD는 확률적 경사하강법이라고 하여 mini batch라고 하는 전체 데이터셋에서 확률적으로 선택된 소규모 데이터 샘플 그룹을 사용하여 각 단계에서 경사를 계산하고 가중치를 업데이트 하는 방식으로 작동한다.

   mini batch 내의 각 데이터 포인트에 대한 손실을 계산하고 그에 대한 가중치의 편미분을 수행한다. 이를 통해 손실 함수의 경사를 산출한다. 이 경사를 이용하여 가중치를 업데이트한다. 이때 하이퍼파라미터로 정해진 학습률이 사용되어 가중치를 얼마나 크게 조정할 지 결정한다. ~~가중치의 가중치~~ mini batch 단위로 이 과정을 반복하여 모델을 최적화할 수 있다.

   업데이트 식은 다음과 같으며 이는 경사하강법과 동일하다. SGD와 경사하강법의 차이는 오직 입력된 데이터에서만 존재한다.

   > $x_{t+1}$ $=$ $x_t - η \frac{∂f}{∂x}(x_t)$

   기존의 경사하강법은 전체 데어테셋을 바탕으로 진행하기에 학습 수렴속도가 느리다는 단점이 있었지만, 이 방법은 대량의 데이터에 대한 훈련을 빠르게 수행할 수 있게 한다. 하지만 mini batch의 크기(batch size)와 학습률에 따라 모델 성능에 큰 영향을 받는다는 단점이 있다.

2. Momentum

   Momentum은 기존 경사하강법에 가속도항을 추가하여 local minimum 문제를 해결한 경사하강 방법론이며 이의 업데이트 식은 다음과 같다.

   > $v_{t+1}=mv_t-η\frac{∂L}{∂W}$

   > $W_{t+1} = W_t + v_t$

   $v$는 일종의 가속도라고 생각하는 것이 이해가 편하다. $v$의 영향으로 인해 기존 가중치가 이전 업데이트 방향으로 더 크게 변화하게끔 하였다. 당연히 $v$는 처음에 0으로 초기화된다.

   또한 $m$은 momentum 운동량 또는 momentum 계수라고 하며, 이를 통해 업데이트가 양의 방향와 음의 방향을 순차적으로 오가며 일어나는 지그재그 현상이 줄어들고, 이전 이동을 고려하여 일정 비율만큼 다음 값을 결정하기에 관성의 효과를 낼 수 있다. 미분계수가 0인 지점에 도달하여도 관성 덕분에 계속 업데이트가 될 수 있다.

3. Adagrad

   adagrad(ADAptive GRADient descent)는 적응형 하강법이라는 뜻이다. 각각의 매개변수에 대해 학습률을 동적으로 조절해주는 원리이다. adagrad는 많이 변화한 변수에 대해서는 학습률을 적게 하고, 그렇지 않은 변수에 대해서는 학습률을 크게 한다. 많이 변화한 변수는 최소점에 가까워졌을 것이라고 생각하고 세밀하게 조정하며, 반대로 적게 변화한 변수는 학습률을 크게 하여 loss를 줄인다.

   업데이트 식은 다음과 같다. 이때 $ϵ$는 0으로 나눠지는 것을 방지하기 위한 상수이며, 보통 $10^{-8}$이다.

   > $x_{t+1} = x_t - \frac{η}{\sqrt{G_t+ϵ}}\frac{∂f}{∂x}(x_t)$

   다음은 기울기를 누적하는 식이며, 처음에 0으로 초기화된다.

   > $G_t=G_{t−1}+(\frac{∂f}{∂x}(x_t))^2$

   위 두 식을 합쳐서 다음과 같이 표현한다.

   > $x_{t+1} = x_t - \frac{η}{\sqrt{\displaystyle\sum^t_{i=1}{(\frac{∂f}{∂x}(x_i))^2}+ϵ}}\frac{∂f}{∂x}(x_t)$

4. RMSProp

5. Adam
