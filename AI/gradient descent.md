# 경사하강법 탐구하기

> 경사하강법은 함수의 최솟값을 찾는 optimizer 이다.

~~인공신경망을 만만하게 본 학우들이 처음으로 갈려나가는 관문이다.~~

optimizer 는 최적화 이론 기법이라고 할 수 있다.

---

## 개념

1. 정의

   경사하강법이란 함수의 값이 낮아지는 방향으로 각 독립변수들의 값을 변형시키면서 함수가 최솟값을 갖도록 하는 독립변수의 값을 탐색하는 방법이다. 함수의 gradient(경사)를 구하고, 해당 gradient의 반대 방향으로 이동시킨다. 이 과정을 극값, 즉 최소 지점에 이를 때 까지 반복한다. 이를 반대로 최고 지점에 이를 때 까지 반복하면 경사상승법이라고 한다.

2. gradient

   여기서 gradient란 $R^n$ -> $R$ 형태의 실수 함수의 미분을 뜻한다.

   함수값이 실수인 $n$차원 다변수함수 $f(x)=f(x_1, x_2, ..., x_n)$가 있을 때, **$f(x)$를 각 변수에 대해 일차 편미분한 벡터**를 gradient라고 하며 매개변수가 $x$일 때 목적함수 $f$의 gradient 값을 $∇f(x)$와 같이 표현한다. 따라서 gradient는 $x$에 대해 $f(x)$가 가장 가파르게 증가하는 방향과 증가율을 나타낸다고 할 수 있다.

---

## 알고리즘

기본적인~~아주 이상적인 데이터의~~ 경사하강법의 알고리즘은 다음 순서를 거친다.

1. 시작점

   임의의 매개변수를 시작점으로 설정한다.

   이 시작점을 어디로 잡느냐에 따라 나중에 수렴 지점이 달라질 수 있다. $y$값이 가장 낮은 부분인 전역 최소점(global minima)을 목적으로 했지만, 미분계수가 가장 낮은 부분인 지역 최소점(local minima)으로 수렴할 가능성이 있다. 이러한 문제를 local minimum 문제라고 한다. 따라서 수렴 지점이 무조건 최소값이 아닐 수 있다.
   ![Untitled](https://github.com/user-attachments/assets/bb8e8ebd-7dce-408b-8075-05cc73504846)

2. 업데이트

   최적화하고자 하는 함수 $f(x)$에 대해 다음과 같은 식을 이용해 다음 가중치의 위치로 이동 하며, 이를 업데이트라고 한다. 이때 $x_t$는 $t$번째 반복의 매개변수 $x$값이다.

   > $\$x_{t+1}$ $=$ $x_t - η \frac{∂f}{∂x}(x_t)$

   이를 다변수함수에 대해 확장하면 다음과 같다.

   > $x_{t+1}$ $=$ $x_t - η∇f(x_t)$

   미분의 인수가 함수 $f$의 그것과 ~~편미분이니까 당연히~~같으므로, 함수 의존성을 명시적으로 표기하여 정의하자면 다음과 같다.

   > $∇f(x_1, x_2, ..., x_n) = (\frac{∂f}{∂x_1},\frac{∂f}{∂x_2},...,\frac{∂f}{∂x_n}) = \frac{∂f}{∂x}(x_1, x_2,...,x_n)$

   이때 계수 $η$는 이동할 거리(step size)를 조정하는 하이퍼파라미터이며, 이를 학습률(learning rate)이라고 한다. 학습률이 너무 크면 수렴할 값을 놓치고 발산할 수 있고(overshooting), 학습률이 너무 작으면 그만큼 수렴이 늦어질 수 있다.

   다음은 각각 학습률이 너무 클때(overshooting)와 너무 낮을때(learn too slow)를 표현한 그림이다.

   ![Untitled2](https://github.com/user-attachments/assets/4d799835-d665-4575-af65-75ff74b592e9)

3. 반복

   목적함수가 특정 값으로 알맞게 수렴할 때 까지 이 과정을 반복한다. 인공신경망에서는 손실함수가 최소가 되는 지점을 찾기 위해 경사하강법을 사용한다.

---

## 종류

필자의 능지 이슈로 인해 단일 변수 함수에 대한 편미분 수식만 기술했다.

1. SGD

   SGD는 Stochastic Gradient Descent의 약자로, 확률적 경사하강법이라고 하여 mini batch라고 하는 전체 데이터셋에서 확률적으로 선택된 소규모 데이터 샘플 그룹을 사용하여 각 단계에서 경사를 계산하고 가중치를 업데이트 하는 방식으로 작동한다.

   mini batch 내의 각 데이터 포인트에 대한 손실을 계산하고 그에 대한 가중치의 편미분을 수행한다. 이를 통해 손실 함수의 경사를 산출하고, 이 경사를 이용하여 가중치를 업데이트한다. 이때 하이퍼파라미터로 정해진 학습률이 사용되어 가중치를 얼마나 크게 조정할 지 결정한다. ~~가중치의 가중치~~ mini batch 단위로 이 과정을 반복하여 모델을 최적화할 수 있다.

   업데이트 식은 다음과 같으며 이는 경사하강법과 동일하다. SGD와 경사하강법의 차이는 오직 입력된 데이터에서만 존재한다.

   > $x_{t+1}$ $=$ $x_t - η \frac{∂f}{∂x}(x_t)$

   기존의 경사하강법은 full batch를 바탕으로 진행하기에 학습 수렴속도가 느리다는 단점이 있었지만, 이 방법은 대량의 데이터에 대한 훈련을 빠르게 수행할 수 있게 한다. 하지만 mini batch의 크기(batch size)와 학습률에 따라 모델 성능에 큰 영향을 받는다는 단점이 있다.

2. Momentum

   Momentum은 기존 경사하강법에 가속도항을 추가하여 local minimum 문제를 해결한 경사하강 방법론이며 이의 업데이트 식은 다음과 같다.

   > $v_{t+1}=mv_t-η\frac{∂L}{∂W}$

   > $W_{t+1} = W_t + v_t$

   $v$는 일종의 가속도라고 생각하는 것이 이해가 편하다. $v$의 영향으로 인해 기존 가중치가 이전 업데이트 방향으로 더 크게 변화하게끔 하였다. 당연히 $v$는 처음에 0으로 초기화된다.

   또한 $m$은 momentum 운동량 또는 momentum 계수라고 하며, 이를 통해 업데이트가 양의 방향와 음의 방향을 순차적으로 오가며 일어나는 지그재그 현상이 줄어들고, 이전 이동을 고려하여 일정 비율만큼 다음 값을 결정하기에 관성의 효과를 낼 수 있다. 미분계수가 0인 지점에 도달하여도 관성 덕분에 계속 업데이트가 될 수 있다.

3. Adagrad

   Adagrad(ADAptive GRADient descent)는 적응형 하강법이라는 뜻이다. 각각의 매개변수에 대해 학습률을 동적으로 조절해주는 원리이다. adagrad는 많이 변화한 변수에 대해서는 학습률을 적게 하고, 그렇지 않은 변수에 대해서는 학습률을 크게 한다. 많이 변화한 변수는 최소점에 가까워졌을 것이라고 생각하고 세밀하게 조정하며, 반대로 적게 변화한 변수는 학습률을 크게 하여 loss를 줄인다.

   업데이트 식은 다음과 같다. 이때 $ϵ$는 0으로 나눠지는 것을 방지하기 위한 상수이며, 보통 $10^{-4}$ 에서 $10^{-8}$ 사이의 값을 취한다.

   > $x_{t+1} = x_t - \frac{η}{\sqrt{G_t+ϵ}}\frac{∂f}{∂x}(x_t)$

   다음은 기울기를 누적하는 식이며, 처음에 0으로 초기화된다.

   > $G_t=G_{t−1}+(\frac{∂f}{∂x}(x_t))^2$

   위 두 식을 합쳐서 다음과 같이 표현하기도 한다.

   > $x_{t+1} = x_t - \frac{η}{\sqrt{\displaystyle\sum^t_{i=1}{(\frac{∂f}{∂x}(x_i))^2}+ϵ}}\frac{∂f}{∂x}(x_t)$

   Adagrad의 장점은 학습률을 신경쓰지 않아도 된다는 것에 있다. 하지만 학습을 계속 진행함에 따라 step size가 지나치게 줄어들어 거의 움직이지 않는 상태가 된다는 단점이 있다. 앞선 식에서 알 수 있듯, $G$에서 계속 제곱된 값을 할당해주기 때문에 $G$의 값들은 계속 빠르게 증가하기 때문이다.

4. RMSProp

   RMSProp은 adagrad의 $G_t$ 값이 무한히 커지는 문제를 지수 가중 이동 평균을 이용해 방지한 방법론이다. $G_t$를 합이 아니라 지수 가중 이동 평균으로 대체함으로써, $G_t$가 최근 변화량의 변수간 상대적인 크기 차이를 유지한다.

   지수 가중 이동 평균이란, 최근 값을 예전 값보다 더 영향력있게 반영하기 위해 최근 값과 예전 값에 각각 적절한 가중치를 주어 계산하는 방법이라고 할 수 있다. 이해하기 쉽게 식으로 표현하자면 다음과 같다.

   > $x_t = αp_t + (1-α)x_{t-1}$

   $t$번째 step에서의 지수 이동평균값 $x_t$에서 현재 값을 $p$, 가중치를 $α$로 표현하였다. 예를 들어 가중치 $α$의 값을 다음과 같이 표현할 수 있다.

   > $α = \frac{2}{t+1}$

   $t$는 값의 개수라고도 할 수 있으며, $t$가 0일 경우를 대비하여 1을 더해준 모습이다. 가중치 $α$는 $t$가 작을 수록 커질 것이다. 필터이론에서는 이러한 가중치를 forgetting factor 또는 decaying factor라고 한다. 이를 적용한 RMSProp의 수식은 다음과 같다.

   > $x_{t+1} = x_t - \frac{η}{\sqrt{E[g^2]_t+ϵ}}\frac{∂f}{∂x}(x_t)$

   이 식에서 분모에 있는 기울기 제곱의 지수 가중 이동 평균 $E[g^2]_t$를 수식으로 정의하면 다음과 같다.

   > $E[g^2]_t=γE[g^2]_{t-1}+(1-γ)(\frac{∂f}{∂x}(x_t))^2$

   여기서 ϵ는 adagrad의 그것과 같은 용도이고, $γ$는 기울기 제곱의 가중치를 조절하는 감쇠 계수로, 보통 0.9정도로 설정된다.

5. Adam

   Adam(ADAptive Moment estimation)은 RMSProp과 momentum 방식을 결합한 알고리즘이다.

   우선 momentum처럼 지금까지 계산한 기울기와

   > $g_t=∇_xJ(x_{t-1})$

   그의 지수 평균을 저장하고,

   > $m_t=β_1m_{t-1}+(1-β_1)g_t$

   RMSProp의 기울기 제곱의 지수 가중 이동 평균을 저장한다.

   > $m_t=β_1m_{t-1}+(1-β_1)g_t^2$

   다만 adam에서는 다음과 같이 초기화가 이루어지기 때문에

   > $m = 0$

   > $v = 0$

   학습 초반에 $m_t$와 $v_t$가 0에 가깝게 bias되어 있을 것이라고 추정하고 이를 보정하는 과정을 거친다.

   > $m_t=\frac{m_t}{1-β_1^t}$

   > $v_t=\frac{v_t}{1-β_2^t}$

   최종적인 파라미터의 업데이트 식은 다음과 같다.

   > $x_t=x_{t-1}-\frac{η}{\sqrt{v_t}+ϵ}m_t$

   여기서 $β_1$과 $β_2$는 1차, 2차 모먼트 추정치의 지수적 감쇠율이며, 보통 각각 0.9, 0.999를 취한다.

---
